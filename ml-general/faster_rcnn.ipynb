{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNNまとめ\n",
    "\n",
    "## 背景\n",
    "- Fast R-CNNではRegion ProposalでSelective Searchというアルゴリズムを使っていたので計算に時間がかかった。\n",
    "- CNNから得られる特徴マップも使って計算を共通化して効率よく領域を算出したい。\n",
    "\n",
    "## 全体構成\n",
    "1. Conv Layerで特徴マップを得る\n",
    "2. Region Proposal Network(RPN)で領域(Region Proposal)を抽出する\n",
    "3. Conv Layerで得られた特徴マップをRoIで切り出す\n",
    "4. 切り出した領域を分類する\n",
    "\n",
    "![](https://cdn-ak.f.st-hatena.com/images/fotolife/t/tereka/20180303/20180303132456.png)\n",
    "\n",
    "一枚の画像の推論時間\n",
    "\n",
    "Fast R-CNN:2.3秒\n",
    "\n",
    "Faster R-CNN:0.2秒\n",
    "\n",
    "## Region Proposal Network\n",
    "\n",
    "### 概要・目的\n",
    "入力: 特徴マップ\n",
    "出力: 矩形領域とobjectness score\n",
    "- 物体領域の候補を計算\n",
    "- End-to-Endな処理によって高速化\n",
    "- 3~4層ほどのCNNで構成可能\n",
    "\n",
    "box-regression layer($reg$)とbox-classification layer($cls$)にそれぞれ特徴マップが入力される。\n",
    "![](https://cdn-ak.f.st-hatena.com/images/fotolife/t/tereka/20180303/20180303132658.png)\n",
    "\n",
    "$k$は各地点の最大のProposalの数を表す。\n",
    "- 矩形を表す座標は$4k$個なので$reg$レイヤの出力数は$4k$\n",
    "- $cls$レイヤの出力が2クラスのソフトマックス層(物体か物体でないかを判別する)なので出力数は$2k$(ロジスティック回帰を用いれば$k$個)\n",
    "\n",
    "$k$個のAnchorを元にproposalを生成。\n",
    "\n",
    "処理の流れ\n",
    "1. 物体の候補領域を見つけるAnchorの生成\n",
    "2. 学習のための誤差を計算\n",
    "3. 候補の中からNMSを用いて有効な領域を絞る\n",
    "\n",
    "### Anchor\n",
    "Sliding windowの中心点を各Anchorの中心点とする\n",
    "\n",
    "スケールとアスペクト比を与えて生成する\n",
    "- デフォルトだと3スケール、3アスペクト比で合計$k=9$になる。\n",
    "- $W * H$のサイズを持つ特徴マップであればAnchorは合計で$WHk$個\n",
    "\n",
    "![](https://camo.qiitausercontent.com/bc1839c4c56095cdb9ccfdedb02f419a7cd3af67/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3137313931352f34646538303631372d313462372d653937302d663130382d6531383366306231646133342e706e67)\n",
    "\n",
    "### 損失関数\n",
    "\n",
    "RPNの訓練において、各Anchorに2値のラベル（物体があるかないか）が付加される。\n",
    "- Anchorとground-truthのIoU(Intersection over Union)が0.7以上 => 物体が存在(Positive anchor)\n",
    "- Anchorとground-truthのIoUが0.3以下 => 物体が存在しない(Negative anchor)\n",
    "- 一つのground-truth boxで複数のAnchorにクラスを付加できる\n",
    "- これら以外は学習に貢献しない\n",
    "\n",
    "$$\n",
    "L(\\{p_i\\}, \\{t_i\\}) = \\frac{1}{N_{cls}}\\sum_{i}L_{cls}(p_i, p^*_i) + \\lambda\\frac{1}{N_{reg}}\\sum_{i}p^*_iL_{reg}(t_i, t^*_i)\n",
    "$$\n",
    "\n",
    "$i$: Anchorのインデックス\n",
    "\n",
    "$p_i$: $i$番目のAnchorが物体領域である確率\n",
    "\n",
    "$p^*_i$: $i$番目のAnchorに対するground-truthラベル(物体領域であるときは1)\n",
    "\n",
    "$t_i$: 推測されたbounding boxを表すベクトル（4点の座標）\n",
    "\n",
    "$t^*_i$: ground-truth boxの座標\n",
    "\n",
    "$p^*_{i}L_{reg}$はanchorがpositiveのときにだけ$L_{reg}$を計算することを意味する。（$p^*_{i} = 0$のときは物体領域でないので）\n",
    "\n",
    "$reg$レイヤと$clf$レイヤの出力はそれぞれ$\\{t_i\\}$、$\\{p_i\\}$になる。\n",
    "\n",
    "$\\lambda$はanchorのミニバッチをとった際の$clf$と$reg$の出力個数の違いを反映し、両者が同じ影響力を持つようにするための正規化パラメータ（デフォルトで$\\lambda = 10$）\n",
    "\n",
    "### 訓練\n",
    "- positive anchorとnegative anchorの割合が等しくなるように各画像に対してanchorのミニバッチを取得しながら訓練していく。\n",
    "\n",
    "\n",
    "### Non-maximum Supression(NMS)\n",
    "- IoUの閾値を決めて、同じ物体を示している領域の抽出を行う。\n",
    "- IoUが小さい領域は削除する\n",
    "\n",
    "### Rol Pooling\n",
    "可変長である抽出RoIを分類ネットワークに入力できるように固定次元の特徴マップに変換 \n",
    "![](https://camo.qiitausercontent.com/8cf05cf36d952981add1dbf70b0cafa01731adfe/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3133393830392f63383161356163372d663135392d303835382d653162642d6537326664343262646363392e706e67)\n",
    "\n",
    "元画像のRegion Proposalの領域をfeature map上に投影すると、feature mapとサブピクセルレベルのズレが生じる。\n",
    "\n",
    "よって以下の処理を施す。（3x3のfeature mapを得たいとする）\n",
    "![](https://camo.qiitausercontent.com/ab088881a617422c99b40610f357cad4ac0037b7/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3133393830392f39616366363937312d363263332d383265312d656136312d3562613062623736393862312e706e67)\n",
    "1. Region Proposalの座標を整数値に丸め込み、赤い外接矩形を得る。\n",
    "2. 得たいfeature mapのサイズと同じビン数で分割\n",
    "3. 元のfeature map内のピクセルを3x3のビンのいずれかに割り当て、maxやaverageを取る。\n",
    "\n",
    "\n",
    "## 参考文献\n",
    "https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n",
    "http://nonbiri-tereka.hatenablog.com/entry/2018/03/07/075835\n",
    "\n",
    "https://qiita.com/arutema47/items/8ff629a1516f7fd485f9\n",
    "\n",
    "https://qiita.com/yu4u/items/5cbe9db166a5d72f9eb8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
