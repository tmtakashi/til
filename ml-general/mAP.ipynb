{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP(mean Average Precision)\n",
    "\n",
    "物体検知モデルに使われる評価指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision（適合率）\n",
    "予測がどれだけ正確かを表す。\n",
    ">光の速さを知りたいとします。\n",
    ">\n",
    ">何らかの検索システムで「光の速さ」で検索した結果、100件の文書がヒットしたとする。\n",
    ">\n",
    ">その100件のうち、「光の速さ」が正しく分かる文書を正解とし、正解は60件だったとする。\n",
    ">\n",
    ">すると、この場合の適合率は 60/100 = 0.60 となる。\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall（再現率）\n",
    "結果として出てくるべきもののうち、実際出てきたものの割合\n",
    ">検索システムが扱う全データ(文書)の中で、光の速さが分かるものは全部で200件だとする。\n",
    ">\n",
    ">しかし、「光の速さ」と検索して、実際に得られた結果(文書)は90件だとする。\n",
    ">\n",
    ">すると、この場合の再現率は 90/200 = 0.45 となる。\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IoU（Intersection over Union）\n",
    "予測結果とGround Truthがどれだけ重なっているかを表す。\n",
    "\n",
    "閾値(0.5など)を設定し、予測がTrue PositiveかFalse Positiveかを判断する場合もある。\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*FrmKLxCtkokDC3Yr1wc70w.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AP (Average Precision、平均適合率)\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*9ordwhXD68cKCGzuJaH2Rg.png\" width=\"500\">\n",
    "\n",
    "上の表は、5つのりんごを各画像に含むデータセットにおいて、モデルの予測結果を予測の信頼度順で並べたものである。\n",
    "\n",
    "2つめのカラム（Correct?）は予測が正しいかを表す（この例では$IoU\\geq0.5$で正しいとされる）\n",
    "\n",
    "まず、Rankが3の列を例に、precisionとrecallがどのように計算されているかを見てみる。\n",
    "\n",
    "1. Precision\n",
    "画像内のりんごの領域を3つ予測したのに対し、実際は２つしか正しく予測できていない => 2/3 = 0.67\n",
    "\n",
    "2. Recall\n",
    "画像内に５つりんごがあるのに対し、正しく予測できているのは２つのみ => 2/5 = 0.4\n",
    "\n",
    "Recallは表の下に行くほど上がるが、precisionはzigzagなパターンで動く。\n",
    "\n",
    "これを可視化するために横軸にRecall、縦軸にPrecisionをとってプロットする\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*VenTq4IgxjmIpOXWdFb-jg.png\" width=\"500\">\n",
    "\n",
    "APの定義は、上のprecision-recall曲線の下の部分の面積である。\n",
    "$$\n",
    "AP = \\int_0^1p(r)dr\n",
    "$$\n",
    "\n",
    "PrecisionとRecallは常に0から1の間値を取るため、APも常に0から1の間の値をとる。\n",
    "\n",
    "APを物体検知について計算する前に、しばしば上のzigzagパターンをなだらかにする。\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*zqTL1KW1gwzion9jY8SjHA.png\" width=\"500\">\n",
    "\n",
    "各recallの値に対して、右側にある一番大きいprecisionの値をとってきてそのrecallに対するprecisionの値と置き換える。\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*pmSxeb4EfdGnzT6Xa68GEQ.jpeg\" width=\"500\">\n",
    "\n",
    "オレンジの線は緑の線に変換され、zigzagなパターンから単調に減少するパターンになる。\n",
    "\n",
    "この変換により計算されたAPの値はランクのわずかな変動に対して安定するようになる。\n",
    "\n",
    "数学的には、再現率$\\tilde r$に対する適合率を、$\\tilde r$より右側の範囲にある最大の適合率で置き換える。\n",
    "\n",
    "$$\n",
    "p_{interp}(r) = \\max_{\\tilde r \\geq r}p(\\tilde r)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolated AP（補完適合率）\n",
    "\n",
    "PASCAL VOC challengeにおいて、予測は$IoU \\geq 0.5$のときPositiveになる。また、同じ物体に対して複数の検出がなされた場合、一番最初の検出をPositiveとし、残りをNegativeとする。\n",
    "\n",
    "Pascal VOC2008では、11点の補完適合率の平均が計算された。\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*naz02wO-XMywlwAdFzF-GA.jpeg\" width=\"500\">\n",
    "\n",
    "具体的には、まずrecallの値を0から1.0まで11点に分ける(0, 0.1, 0.2, ..., 0.9, 1.0)。\n",
    "\n",
    "次に、これら11個のrecallの値に対してmaximum precision value（上の$p_iterp(r)$）の平均を計算する。\n",
    "\n",
    "$$\n",
    "AP = \\frac{1}{11} \\sum_{r \\in (0.0, \\dots, 1.0)}AP_r \\\\\n",
    " = \\frac{1}{11} \\sum_{r \\in (0.0, \\dots, 1.0)}p_iterp(r) \\\\\n",
    " = \\frac{1}{11} (AP_r(0) + AP_r(0.1) + \\dots + AP_r(1.0))\n",
    "$$\n",
    "\n",
    "$AP_r$が非常に小さい時、残りの項も0であるとみなせ、recallが100%になるまで予測をする必要がないということがわかる。\n",
    "\n",
    "PASCAL VOCにはクラスが20個ありAPはそれぞれのクラスに対して計算され、これらの平均がmAPとして算出される。\n",
    "\n",
    "この手法には問題点が２つある。\n",
    "\n",
    "    1. あまり正確ではない\n",
    "    2. 低いAPに対しての手法の違いを測れない\n",
    "    \n",
    "よってPASCAL VOCは2008年以降、異なるAPの計算手法が採用されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AP（Area under curve, AUC）\n",
    "\n",
    "VOC2010-2012では、maximum precision valueが下がるようなすべてのユニークな再現率$(r1, r2, \\dots)$についてAPを計算する。\n",
    "\n",
    "この変更により、zigzagが取り除かれたprecision-recall曲線の下の面積を正確に計算している。\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*TAuQ3UOA8xh_5wI5hwLHcg.jpeg\" width=\"500\">\n",
    "\n",
    "11点サンプリングする代わりに、$p(r_i)$が下がることにそれをサンプリングし、APを長方形ブロックの面積の和として計算する。\n",
    "\n",
    "$$\n",
    "AP = \\sum (r_{n+1} - r_n)p_{interp}(r_{n+1})\\\\\n",
    "p_{interp}(r_{n+1}) = \\max_{\\tilde r \\geq r_{n+1}} p(\\tilde r)\n",
    "$$\n",
    "\n",
    "この定義はArea Under Curve(AUC)と呼ばれている。\n",
    "\n",
    "以下に示すように、補完された点はprecisionが落ちる点を含んでいないため、これらの手法が異なることがわかる。\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*dEfFSY6vFPSun96lRoxOEw.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO mAP\n",
    "\n",
    "最新の研究ではCOCO datasetに対する結果のみ示す傾向がある。\n",
    "\n",
    "- COCO mAPでは101点の補完適合率が計算に使われている。\n",
    "- 複数のIoUの閾値に関しての平均をAPとしている。\n",
    "    - 例えばAP@[.5:.95]はステップサイズ0.05で0.5から0.95までのIoUのAPの平均をとっている。\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*09w7--mzDQ3Nqd81t-XOjw.png\" width=\"500\">\n",
    "\n",
    "上の図の$AP_{75}$はIoU=0.75でのAPを表す。\n",
    "\n",
    "\n",
    "mAPはAPの平均である。ある文脈では各クラスのAPを計算し、それらを平均する。しかし他の文脈ではAPとmAPは同じ場合もある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\n",
    "\n",
    "http://www.cse.kyoto-su.ac.jp/~g0846020/keywords/keywordsTop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
